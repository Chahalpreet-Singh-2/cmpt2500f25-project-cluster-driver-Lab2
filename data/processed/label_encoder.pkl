# **3**

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

# Specify the path to your CSV file
csv_file_path = 'CBB_Listings.csv'

# Read the dataset into a Pandas DataFrame
df = pd.read_csv(csv_file_path,on_bad_lines='skip')

df.head()

df.info()

#Droping columns
df = df.drop(['dealer_email', 'dealer_phone', 'has_leather', 'has_navigation'], axis=1)

df.isnull().sum()

# Convert 'listing_first_date' to datetime
df['listing_first_date'] = pd.to_datetime(df['listing_first_date'])

# Calculate 'listing_dropoff_date' using days_on_market columns
df['listing_dropoff_date'] = df['listing_first_date'] + pd.to_timedelta(df['days_on_market'], unit='D')

#updating ''listing_dropoff_date' to datetime
df['listing_dropoff_date'] = pd.to_datetime(df['listing_dropoff_date'])

# Display the updated DataFrame
df[['listing_first_date', 'days_on_market', 'listing_dropoff_date']].head()

#Calculating the number of 0s in 'price' column
num_zeros = (df['price'] == 0).sum()
print(f"Number of 0s in 'price' column: {num_zeros}")\

#calculating the median of the 'price' column excluding 0s
median_price = df[df['price'] != 0]['price'].median()

#filling the 0s with the median
df.loc[df['price'] == 0, 'price'] = median_price

#Displaying price
df['price']

df.isnull().sum()

# Replace the nulls in series, exterior_color, exterior_color_category, interior_color, interior_color_category with mode
df['series'].fillna(df['series'].mode()[0])
df['exterior_color'].fillna(df['exterior_color'].mode()[0])
df['exterior_color_category'].fillna(df['exterior_color_category'].mode()[0])
df['interior_color'].fillna(df['interior_color'].mode()[0])
df['interior_color_category'] = df['interior_color_category'].fillna(df['interior_color_category'].mode()[0])

# Replacing numerical values in Certified column with categorical ones
df['certified'] = df['certified'].replace({0: 'No', 1: 'Yes'})
df['certified']

#Replace 0s with NaN
df['wheelbase_from_vin'] = df['wheelbase_from_vin'].replace(0, np.nan)

#imputaion with median:
df['wheelbase_from_vin'] = df['wheelbase_from_vin'].fillna(df['wheelbase_from_vin'].median())

df['wheelbase_from_vin']

df.info()

#Categorical features from data for table and bar charts
categorical_features = ['series', 'exterior_color', 'exterior_color_category',
                        'interior_color', 'interior_color_category',
                        'drivetrain_from_vin', 'transmission_from_vin',
                        'fuel_type_from_vin', 'engine_from_vin', 'make', 'stock_type']

for feature in categorical_features:
  print(f"Frequency Table for {feature}:\n")
  print(df[feature].value_counts())
  print("\n" + '-'*30 +  "\n")

unique_postal_code = df['dealer_postal_code'].unique()
print(f"Number of unique values: {len(unique_postal_code)}")

import geopy
import pandas as pd
from geopy.extra.rate_limiter import RateLimiter # to avoid overloading the server
from functools import lru_cache #used to cache the results

postal_codes = pd.read_csv('POSTAL_CODE.csv')  # LOad POSTAL code FILE
print(postal_codes.head())

def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Apply IQR method to key numerical columns
columns_to_check = ['price', 'mileage', 'msrp', 'wheelbase_from_vin', 'location_score']
for col in columns_to_check:
    df = remove_outliers_iqr(df, col)


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns



# Choose a specific column to check for outliers (e.g., 'price')
column = 'price'

# Calculate IQR and identify outliers
Q1 = df[column].quantile(0.25)
Q3 = df[column].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

# Print outliers information
print("Outliers detected:")
print(outliers)

# Visualize the boxplot for the column
plt.figure(figsize=(8, 6))
sns.boxplot(x=df[column])
plt.title(f'Boxplot of {column} (Outliers Identified)')
plt.show()


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns



# Choose a specific column to check for outliers (e.g., 'price')
column = 'days_on_market'

# Calculate IQR and identify outliers
Q1 = df[column].quantile(0.25)
Q3 = df[column].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

# Print outliers information
print("Outliers detected:")
print(outliers)

# Visualize the boxplot for the column
plt.figure(figsize=(8, 6))
sns.boxplot(x=df[column])
plt.title(f'Boxplot of {column} (Outliers Identified)')
plt.show()


#finding the outliers using z score on column days on market
from scipy import stats
import numpy as np
#calculating the mean and standard deviation for the column days on market
mean = np.mean(df['days_on_market'])
std_dev = np.std(df['days_on_market'])

#calculating the z score for days on market
df['z_score'] = (df['days_on_market'] - mean) / std_dev
threshold = 3
outliers = df[df['z_score'].abs() > threshold]
print(f"Z-scores:\n{df[['listing_id', 'days_on_market', 'z_score']].head()}")
print(f"\nOutliers:\n{outliers[['listing_id', 'days_on_market', 'z_score']]}")
#PLOTTING THE Boxplot for z score and outlier
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['days_on_market'])
plt.title(f'Boxplot of days_on_market (Outliers Identified)')
plt.show()



!pip install plotly
import plotly.express as px
# Scatter plot for Price vs Days on Market
fig = px.scatter(df,
                 x='days_on_market',  # X-axis: Days on Market
                 y='price',           # Y-axis: Price
                 hover_name='listing_heading',  # Information to show on hover
                 hover_data=['dealer_name', 'dealer_city', 'dealer_province'],  # Additional info on hover
                 title="Price vs Days on Market of Listings",
                 labels={'days_on_market': 'Days on Market', 'price': 'Price (in $)'})

# Display the scatter plot
fig.show()


import plotly.graph_objects as go

# Assuming df contains the data
# Generate the base pie chart data
pie_chart_data = df['make'].value_counts(normalize=True).reset_index()
pie_chart_data.columns = ['make', 'percentage']
pie_chart_data['percentage'] *= 100  # Convert proportions to percentages

# Store a copy of the original data before filtering
original_pie_chart_data = pie_chart_data.copy()

# Filter out makes with less than 3% and group them under 'Other'
other_makes = original_pie_chart_data[original_pie_chart_data['percentage'] < 3]['make'].tolist()
other_percentages = original_pie_chart_data[original_pie_chart_data['percentage'] < 3]['percentage'].tolist()
other_percentage = sum(other_percentages)

pie_chart_data = pie_chart_data[pie_chart_data['percentage'] >= 3]
pie_chart_data = pd.concat([pie_chart_data, pd.DataFrame([{'make': 'Other', 'percentage': other_percentage}])], ignore_index=True)


# Prepare hierarchical data for the Sunburst chart
labels = pie_chart_data['make'].tolist() + other_makes
parents = [''] * len(pie_chart_data) + ['Other'] * len(other_makes)
values = pie_chart_data['percentage'].tolist() + other_percentages

# Create Sunburst chart
fig = go.Figure(go.Sunburst(
    labels=labels,
    parents=parents,
    values=values,
    branchvalues="total",
    textinfo="label+percent parent"
))

# Update layout for better display
fig.update_layout(
    title="Interactive Distribution of Car Makes (Percentage Proportions)",
    margin=dict(t=50, l=0, r=0, b=0)
)

fig.show()

#  Calculate the frequency of each 'make' and find the most and least popular makes
make_counts = df['make'].value_counts()

# Find the most popular (first) and least popular (last) makes
most_popular_make = make_counts.idxmax()
least_popular_make = make_counts.idxmin()

# Step 3: Filter the DataFrame to include only the rows for the most and least popular makes
df_filtered = df[df['make'].isin([most_popular_make, least_popular_make])]

# Step 4: Create the histogram for 'mileage' column for most and least popular makes
fig = px.histogram(df_filtered,
                   x='mileage',  # Variable for the x-axis (Mileage)
                   nbins=30,      # Number of bins in the histogram
                   title="Distribution of Car Mileage (Most and Least Popular Makes)",
                   labels={'mileage': 'Mileage (in km)'},
                   color='make',  # Color by 'make' to differentiate between the two
                   template='plotly_dark')

# Step 5: Show the histogram
fig.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is your original DataFrame with 'mileage' and 'price' columns

# Calculate IQR and identify outliers for 'mileage'
Q1_mileage = df['mileage'].quantile(0.25)
Q3_mileage = df['mileage'].quantile(0.75)
IQR_mileage = Q3_mileage - Q1_mileage
lb1 = Q1_mileage - 1.5 * IQR_mileage
ub1 = Q3_mileage + 1.5 * IQR_mileage
outliers_feature1 = df[(df['mileage'] < lb1) | (df['mileage'] > ub1)]

# Calculate IQR and identify outliers for 'price'
Q1_price = df['price'].quantile(0.25)
Q3_price = df['price'].quantile(0.75)
IQR_price = Q3_price - Q1_price
lb2 = Q1_price - 1.5 * IQR_price
ub2 = Q3_price + 1.5 * IQR_price
outliers_feature2 = df[(df['price'] < lb2) | (df['price'] > ub2)]

# Create a new DataFrame 'df_simple_outliers' containing only the relevant columns
df_simple_outliers = df[['mileage', 'price']].copy()

# Set Seaborn style for better visuals
sns.set(style="whitegrid")

# Boxplot for 'mileage' with IQR bounds
plt.figure(figsize=(10, 6))
sns.boxplot(x=df_simple_outliers['mileage'], color='lightseagreen')
plt.axvline(lb1, color='darkred', linestyle='--', label='Lower Bound')
plt.axvline(ub1, color='darkblue', linestyle='--', label='Upper Bound')
plt.title('Boxplot for Mileage with IQR Bounds', fontsize=16, fontweight='bold', color='darkblue')
plt.xlabel('Mileage (in km)', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.legend(loc='upper left', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Boxplot for 'price' with IQR bounds
plt.figure(figsize=(10, 6))
sns.boxplot(x=df_simple_outliers['price'], color='cornflowerblue')
plt.axvline(lb2, color='darkorange', linestyle='--', label='Lower Bound')
plt.axvline(ub2, color='darkgreen', linestyle='--', label='Upper Bound')
plt.title('Boxplot for Price with IQR Bounds', fontsize=16, fontweight='bold', color='darkblue')
plt.xlabel('Price (in dollars)', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.legend(loc='upper left', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Scatter plot of 'mileage' vs 'price' with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df_simple_outliers, x='mileage', y='price', color='darkslategray', alpha=0.5, s=50, label='Data Points')

# Highlight outliers with different colors
plt.scatter(outliers_feature1['mileage'], outliers_feature1['price'], color='firebrick', label='Outliers in Mileage', s=100, edgecolor='black', marker='o')
plt.scatter(outliers_feature2['mileage'], outliers_feature2['price'], color='mediumvioletred', label='Outliers in Price', s=100, edgecolor='black', marker='o')

# Add titles, axis labels, and grid
plt.title('Scatter Plot of Mileage vs Price with Outliers Highlighted', fontsize=18, fontweight='bold', color='darkblue')
plt.xlabel('Mileage (in km)', fontsize=14)
plt.ylabel('Price (in dollars)', fontsize=14)
plt.legend(loc='upper left', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()


#plot a histrogram of dealer_city
import plotly.express as px

fig = px.histogram(df, x="dealer_city", y="price",
                   histfunc="sum",  # Use 'sum' to aggregate total price for each city
                   title="Total Price Distribution by Dealer City",
                   labels={"price": "Total Price", "dealer_city": "Dealer City"},
                   color_discrete_sequence=['darkgreen'])
fig.show()

import plotly.express as px
import plotly.graph_objects as go

# Step 1: Find the top 10 most sold car makes
top_10_makes = df['make'].value_counts().nlargest(10).index

# Step 2: Filter the DataFrame to include only the top 10 makes
filtered_df = df[df['make'].isin(top_10_makes)]

# Step 3: Group data by city and make, then count occurrences
city_make_counts = filtered_df.groupby(['dealer_city', 'make'])['make'].count().reset_index(name='count')

# Step 4: Create the histogram
fig_histogram = px.histogram(city_make_counts,
                             x='dealer_city',
                             y='count',
                             color='make',
                             title="Sales Distribution of Top 10 Car Makes Across Cities",
                             labels={'dealer_city': 'City', 'count': 'Number of Car Models Sold'},
                             template='plotly_dark',
                             color_discrete_sequence=px.colors.qualitative.Set3)

fig_histogram.update_layout(barmode='stack')
fig_histogram.show()

# Step 5: Create the bar chart for total counts
total_counts = filtered_df['make'].value_counts().reset_index()
total_counts.columns = ['make', 'total_count']

fig_bar = px.bar(total_counts,
                  x='make',
                  y='total_count',
                  title="Total Sales of Top 10 Car Makes",
                  labels={'make': 'Car Make', 'total_count': 'Total Sales'},
                  template='plotly_dark',
                  color_discrete_sequence=px.colors.qualitative.Set3)

fig_bar.show()


import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set(style="whitegrid")

# Create a subset DataFrame for Edmonton listings
df_edmonton = df[df['dealer_city'] == 'Edmonton']  # Filter for Edmonton

# Plot 1: Vehicle count by postal code
plt.figure(figsize=(12, 6))
df_edmonton["dealer_postal_code"].value_counts().head(15).plot(kind="bar", color="royalblue")
plt.title("Top 15 Postal Codes with Most Listings in Edmonton")
plt.xlabel("Postal Code")
plt.ylabel("Number of Listings")
plt.xticks(rotation=45)
plt.show()

# Plot 2: Price distribution
plt.figure(figsize=(10, 5))
sns.histplot(df_edmonton["price"], bins=50, kde=True, color="green")
plt.title("Price Distribution of Vehicles in Edmonton")
plt.xlabel("Price ($)")
plt.ylabel("Count")
plt.show()

# Plot 3: Days on Market distribution
plt.figure(figsize=(10, 5))
sns.histplot(df_edmonton["days_on_market"], bins=50, kde=True, color="purple")
plt.title("Days on Market Distribution")
plt.xlabel("Days on Market")
plt.ylabel("Count")
plt.show()

# 1. Box Plot: Price distribution by Make
plt.figure(figsize=(14, 6))
top_makes = df_edmonton["make"].value_counts().head(10).index  # Select top 10 makes
sns.boxplot(data=df_edmonton[df_edmonton["make"].isin(top_makes)], x="make", y="price", palette="coolwarm")
plt.title("Price Distribution by Vehicle Make")
plt.xlabel("Make")
plt.ylabel("Price ($)")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=df_edmonton,
    x="days_on_market",
    y="price",
    hue="stock_type",  # Differentiate by stock type
    palette="Set1",    # Use distinct colors
    alpha=0.7
)
plt.title("Days on Market vs. Vehicle Price (by Stock Type)")
plt.xlabel("Days on Market")
plt.ylabel("Price ($)")
plt.legend(title="Stock Type")
plt.show()


# 3. Count Plot: Number of Listings by Model Year
plt.figure(figsize=(12, 6))
sns.countplot(data=df_edmonton, x="model_year", palette="viridis", order=sorted(df_edmonton["model_year"].unique()))
plt.title("Vehicle Listings by Model Year")
plt.xlabel("Model Year")
plt.ylabel("Count of Listings")
plt.xticks(rotation=45)
plt.show()


Heatmap: Correlation Between Numeric Features
( Shows relationships between key numeric features like price, mileage, and days on market.)

plt.figure(figsize=(10, 6))
sns.heatmap(df_edmonton[["price", "mileage", "days_on_market", "model_year"]].corr(), annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap of Key Features")
plt.show()


Bar Chart: Average Price per Stock Type (New vs. Used)
show pricing differences between new and used cars.

plt.figure(figsize=(8, 5))
sns.barplot(data=df_edmonton, x="stock_type", y="price", estimator=sum, palette="pastel")
plt.title("Total Vehicle Sales by Stock Type")
plt.xlabel("Stock Type")
plt.ylabel("Total Sales ($)")
plt.show()


KDE Plot: Mileage Distribution for Best-Selling Postal Codes:analyze the average mileage of cars sold in top-performing regions.

plt.figure(figsize=(10, 6))
top_postal_codes = df_edmonton["dealer_postal_code"].value_counts().head(5).index
for code in top_postal_codes:
    sns.kdeplot(df_edmonton[df_edmonton["dealer_postal_code"] == code]["mileage"], label=code, fill=True)
plt.title("Mileage Distribution for Top 5 Postal Codes")
plt.xlabel("Mileage")
plt.ylabel("Density")
plt.legend(title="Postal Code")
plt.show()


Pair Plot: Price vs. Mileage, Model Year, Days on Market

sns.pairplot(df_edmonton[["price", "mileage", "model_year", "days_on_market"]], diag_kind="kde")
plt.show()


# Check if columns exist before applying get_dummies
columns_to_encode = ['make', 'model', 'dealer_postal_code']
columns_to_encode = [col for col in columns_to_encode if col in df.columns]

# Only apply get_dummies if there are columns to encode
if columns_to_encode:
    df = pd.get_dummies(df, columns=columns_to_encode, drop_first=True)

# Label Encoding for 'condition' (if applicable)
if 'condition' in df.columns:
    le = LabelEncoder()
    df['condition'] = le.fit_transform(df['condition'])

# Check the transformed dataset
print(df.head())

# Save the encoded dataset
df.to_csv('encoded_dataset.csv', index=False)

plt.figure(figsize=(8, 5))
sns.histplot(df['price'], bins=50, kde=True, color='blue')
plt.title("Price Distribution of Vehicles")
plt.xlabel("Price ($)")
plt.ylabel("Frequency")
plt.xlim(0, 200000)  # Focus on relevant price range
plt.show()


plt.figure(figsize=(8, 5))
sns.histplot(df['mileage'], bins=50, kde=True, color='green')
plt.title("Mileage Distribution of Vehicles")
plt.xlabel("Mileage (km)")
plt.ylabel("Frequency")
plt.xlim(0, 300000)  # Exclude extreme outliers
plt.show()


plt.figure(figsize=(8, 5))
sns.histplot(df['days_on_market'], bins=50, kde=True, color='purple')
plt.title("Days on Market Distribution")
plt.xlabel("Days on Market")
plt.ylabel("Frequency")
plt.xlim(0, 200)  # Exclude extreme cases
plt.show()


plt.figure(figsize=(8, 6))
corr = df[['price', 'mileage', 'model_year', 'days_on_market']].corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Key Variables")
plt.show()


# Identify correct postal code column
postal_code_col = None
for col in df.columns:
    if "dealer_postal_code" in col:
        postal_code_col = col
        break

if postal_code_col:
    # Count vehicle sales by postal code
    top_regions = df[postal_code_col].value_counts().head(10)

    plt.figure(figsize=(10, 5))
    sns.barplot(x=top_regions.index, y=top_regions.values, palette="Blues_r")
    plt.xticks(rotation=45)
    plt.title("Top-Selling Regions by Postal Code")
    plt.xlabel("Dealer Postal Code")
    plt.ylabel("Number of Sales")
    plt.show()
else:
    print("Error: Postal code column not found in dataset.")


!pip install category_encoders

import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("CBB_Listings.csv")

# Count missing values before cleaning
missing_before = df.isnull().sum()

# Simulate missing value handling (fill missing values)
df_cleaned = df.fillna(df.median(numeric_only=True))
missing_after = df_cleaned.isnull().sum()




# Count unique categorical values before encoding
cat_cols = ["make", "model", "dealer_city"]
unique_before = sum(df[col].nunique() for col in cat_cols)

# Perform encoding
df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)
unique_after = df_encoded.shape[1] - df.shape[1]  # Number of new columns added

# Pie Chart Data
labels = ["Before Encoding", "After Encoding"]
values = [unique_before, unique_after]
colors = ['purple', 'orange']

# Plot Pie Chart
plt.figure(figsize=(7, 7))
plt.pie(values, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90, wedgeprops={"edgecolor": "black"})
plt.title("Categorical Encoding: Before vs. After")
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Select numerical features
num_cols = ["price", "mileage", "days_on_market"]

# Apply Min-Max Scaling
scaler = MinMaxScaler()
df_scaled = df.copy()
df_scaled[num_cols] = scaler.fit_transform(df[num_cols])

# Plot Before vs. After Scaling
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Before Scaling
sns.histplot(df[num_cols], kde=True, ax=axes[0])
axes[0].set_title("Before Scaling")
axes[0].set_xlabel("Value Range")
axes[0].legend(num_cols)

# After Scaling
sns.histplot(df_scaled[num_cols], kde=True, ax=axes[1], palette="coolwarm")
axes[1].set_title("After Scaling")
axes[1].set_xlabel("Normalized Range")
axes[1].legend(num_cols)

plt.suptitle("Numerical Features: Before vs. After Scaling")
plt.show()


# Count total rows before and after data cleaning
rows_before = df.shape[0]

# Remove rows with missing values & outliers
df_cleaned = df.dropna()  # Removing missing values
df_cleaned = df_cleaned[(df_cleaned["price"] < 100000) & (df_cleaned["mileage"] < 300000)]  # Removing outliers
rows_after = df_cleaned.shape[0]

# Pie Chart Data
labels = ["Rows Removed", "Rows Kept"]
values = [rows_before - rows_after, rows_after]
colors = ["red", "green"]

# Plot Pie Chart
plt.figure(figsize=(7, 7))
plt.pie(values, labels=labels, autopct="%1.1f%%", colors=colors, startangle=90, wedgeprops={"edgecolor": "black"})
plt.title("Dataset Size: Before vs. After Cleaning")
plt.show()


!pip install dash
!pip install dash-leaflet
!pip install geocoder
!pip install plotly
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from scipy import stats
import dash
import dash_html_components as html
import dash_core_components as dcc
import dash_leaflet as dl
from dash.dependencies import Input, Output, State
import geocoder
import plotly.express as px
import plotly.graph_objects as go
from geopy.extra.rate_limiter import RateLimiter # to avoid overloading the server
from functools import lru_cache #used to cache the results



#LOAD POSTAL CODE.csv file
postal_codes = pd.read_csv('POSTAL_CODE.csv')
postal_codes.head(185)

#LOAD POSTAL CODE.csv file
postal_codes = pd.read_csv('POSTAL_CODE.csv')
postal_codes = postal_codes.rename(columns={'dealer_postal_code ': 'dealer_postal_code'})
# 2. Convert dealer_postal_code to string in df
df['dealer_postal_code'] = df['dealer_postal_code'].astype(str)

#

# 1. Create a dictionary of postal codes and coordinates
postal_code_dict = {
    'T5S 1Y8': (53.549233, -113.666536),
    'T6V 1H4': (53.604536, -113.578942),
    'T6H 1B7': (53.492482, -113.493143),
    'T6X 1A1': (53.421713, -113.48973),
    'T8N 7X1': (53.674928, -113.638065),
    'T5L 4H5': (53.591185, -113.564813),
    'T6E 0G7': (53.499461, -113.483908),
    'T8H 2N1': (53.546014, -113.327893),
    'T9E 6T9': (53.281782, -113.546323),
    'T5V 0C3': (53.576926, -113.589133),
    'T5L 5H7': (53.599913, -113.570655),
    'T5J 2E5': (53.544736, -113.491391),
    'T5J 2R7': (53.521746, -113.466265),
    'T5B 2N1': (53.567, -113.464409),
    'T6E 5P2': (53.466121, -113.472256),
    'T8H 0X5': (53.551967, -113.311705),
    'T8H 0R5': (53.556586, -113.310874),
    'T6H 1H1': (53.496786, -113.495747),
    'T6E 6A4': (53.46741, -113.474281),
    'T6H 4J7': (53.48981, -113.493039),
    'T5A 1C3': (53.592721, -113.428304),
    'T5Y 3B2': (53.599576, -113.417911),
    'T9E 0Z7': (53.278779, -113.545601),
    'T6E 5T6': (53.466958, -113.471536),
    'T5S 0A2': (53.559947, -113.62326),
    'T5E 4C7': (53.59468, -113.491331),
    'T5S 1E4': (53.542818, -113.625713),
    'T5B 1K3': (53.579197, -113.443474),
    'T6E 4C6': (53.514708, -113.498031),
    'T6E 5W8': (53.465953, -113.477218),
    'T9E 0W8': (53.333847, -113.542417),
    'T6E 6V6': (53.497962, -113.466073),
    'T5B 1G2': (53.582496, -113.481889),
    'T5S 1X2': (53.552158, -113.683096),
    'T7X 3A6': (53.545537, -113.903261),
    'T5L 4P6': (53.592173, -113.568713),
    'T5S 2C3': (53.542727, -113.694983),
    'T5R 1Y5': (53.520408, -113.589933),
    'T5B 1K2': (53.577026, -113.443418),
    'T6E 5P8': (53.478324, -113.481022),
    'T9E 7A7': (53.274779, -113.547858),
    'T8H 1N1': (53.558652, -113.325833),
    'T6H 2K4': (53.495635, -113.497162),
    'T5S 1S1': (53.542743, -113.631555),
    'T8L 3K8': (53.70586, -113.196872),
    'T6B 0G3': (53.517728, -113.441163),
    'T5S 1M9': (53.543904, -113.630355),
    'T5E 0E2': (53.585057, -113.525642),
    'T7X 6A5': (53.544574, -113.782322),
    'T6A 3A4': (53.542306, -113.444101),
    'T6E 5Z5': (53.486069, -113.477623),
    'T5H 0Y5': (53.552205, -113.51855),
    'T5H 3K1': (53.551929, -113.517376),
    'T6C 4H8': (53.511227, -113.441257),
    'T5B 0S4': (53.570172, -113.469567),
    'T5G 2Z1': (53.582161, -113.492898),
    'T5L 2L5': (53.598442, -113.579851),
    'T6V 1J1': (53.601613, -113.588422),
    'T9E 7L1': (53.289492, -113.54546),
    'T5G 0A8': (53.559383, -113.490767),
    'T5C 3C2': (53.584064, -113.444293),
    'T5L 2J8': (53.570641, -113.532039),
    'T8H 2A2': (53.543867, -113.318065),
    'T5S 1B1': (53.540351, -113.635669),
    'T6E 6G7': (53.468482, -113.489951),
    'T8N 2Y3': (53.619324, -113.6077),
    'T6N 1H8': (53.455708, -113.474548),
    'T9E 7A9': (53.277309, -113.548286),
    'T5S 1A4': (53.541938, -113.630124),
    'T5S 1C6': (53.539442, -113.638367),
    'T5B 0S9': (53.570228, -113.473859),
    'T6W 3G9': (53.428507, -113.590232),
    'T6N 1C1': (53.46373, -113.468913),
    'T5W 1A8': (53.570096, -113.411217),
    'T7X 2Y3': (53.54566, -113.9026),
    'T6E 5X9': (53.467055, -113.482628),
    'T6K 4E7': (53.483413, -113.443877),
    'T5B 4G9': (53.573829, -113.460508),
    'T6X 0S8': (53.426818, -113.489199),
    'T5G 0H8': (53.560534, -113.522931),
    'T6B 2H8': (53.513129, -113.407747),
    'T6E 6J6': (53.469156, -113.481194),
    'T5G 0B1': (53.559035, -113.492552),
    'T5B 1Y2': (53.571448, -113.451741),
    'T6E 3J2': (53.494776, -113.478623),
    'T8H 1B4': (53.554424, -113.319586),
    'T6E 2B1': (53.517704, -113.507252),
    'T6E 0X6': (53.507795, -113.507911),
    'T6E 0K9': (53.50231, -113.488079),
    'T6B 2J8': (53.508366, -113.419273),
    'T5G 1R2': (53.571235, -113.488478),
    'T6E 4P2': (53.498291, -113.459814),
    'T5B 1K1': (53.577413, -113.442568),
    'T5S 1R1': (53.542936, -113.629053),
    'T5V 1K8': (53.577274, -113.586491),
    'T8H 0C7': (53.546738, -113.330088),
    'T6J 6V4': (53.46246, -113.495135),
    'T5S 1P9': (53.542817, -113.63037),
    'T5B 0C4': (53.560843, -113.482261),
     'T5W 1G3': (53.570648, -113.440594),
    'T5B 1N2': (53.580492, -113.444054),
    'T6H 2K8': (53.498717, -113.496086),
    'T5S 2A1': (53.549727, -113.686217),
    'T6E 5A6': (53.489161, -113.474521),
    'T5S 2X1': (53.561611, -113.616362),
    'T5E 6M7': (53.584443, -113.489058),
    'T5S 0B9': (53.54432, -113.641427),
    'T5A 2W9': (53.585779, -113.415452),
    'T5G 3E5': (53.582625, -113.490243),
    'T6B 0B7': (53.513004, -113.441454),
    'T8L 3V5': (53.701823, -113.203532),
    'T5S 1N9': (53.543524, -113.61727),
    'T8A 4N5': (53.513148, -113.325929),
    'T6H 4J8': (53.491305, -113.496239),
    'T8H 0E1': (53.568231, -113.311384),
    'T5C 0A3': (53.58571, -113.443361),
    'T5L 2J7': (53.588151, -113.576664),
    'T8N 3Z7': (53.625671, -113.637513),
    'T5A 1C2': (53.593061, -113.430422),
    'T6W 2P6': (53.422385, -113.497227),
    'T5W 1E5': (53.570063, -113.431555),
    'T6A 0A7': (53.537893, -113.439521),
    'T5E 4C9': (53.59329, -113.49307),
    'T5E 1Y8': (53.598817, -113.494524),
    'T9E 8A5': (53.294746, -113.545184),
    'T6H 5W5': (53.48916, -113.491158),
    'T5B 0S6': (53.570187, -113.470844),
    'T5S 1N4': (53.541463, -113.632697),
    'T5T 3J7': (53.522335, -113.622923),
    'T5H 2S8': (53.556703, -113.494246),
    'T5S 0A1': (53.559087, -113.624689),
    'T8L 0P6': (53.674443, -113.242826),
    'T6E 6S9': (53.467826, -113.47846),
    'T6E 6L1': (53.4683, -113.484201),
    'T6B 2H5': (53.509612, -113.415763),
    'T5S 1P5': (53.542662, -113.628141),
    'T5M 4G4': (53.563623, -113.579631),
    'T5S 2Y4': (53.539532, -113.651114),
    'T5S 1B4': (53.540342, -113.629705),
    'T9E 7C7': (53.282279, -113.546108),
    'T5S 1A7': (53.542411, -113.638397),
    'T6V 1H5': (53.607832, -113.587096),
    'T6E 5R2': (53.478723, -113.477049),
    'T7X 5A3': (53.563063, -113.766949),
    'T6B 2T4': (53.533363, -113.416531),
    'T6X 1E9': (53.422085, -113.474242),
    'T5H 2G1': (53.559576, -113.486317),
    'T6E 4W8': (53.487292, -113.477167),
    'T6B 2Z9': (53.508912, -113.434842),
    'T8H 0M2': (53.56693, -113.313956),
    'T5S 2X6': (53.540246, -113.64364),
    'T8N 3X9': (53.666385, -113.628717),
    'T5S 0J5': (53.559002, -113.61948),
    'T6E 6B3': (53.469841, -113.480067),
    'T5E 4C4': (53.59035, -113.49115),
    'T8N 5A5': (53.636964, -113.570135),
    'T5P 4M9': (53.539746, -113.612409),
    'T5B 4H8': (53.582498, -113.449686),
    'T5M 2R8': (53.559574, -113.596458),
    'T5G 0N8': (53.570173, -113.487994),
    'T6B 3L9': (53.494061, -113.44116),
    'T5S 2C8': (53.545649, -113.622794),
    'T6E 4N7': (53.502571, -113.450468),
    'T9G 1B2': (53.366867, -113.734055),
    'T5L 3C1': (53.582383, -113.550417),
    'T6E 5X8': (53.466829, -113.475741),
    'T5S 0A8': (53.560491, -113.618451),
    'T6H 2H3': (53.495249, -113.491547),
    'T6H 5X8': (53.490051, -113.496852),
    'T5L 4P3': (53.597906, -113.575949),
    'T5H 0Y5': (53.552205, -113.51855),
    'T5L 2Y6': (53.576502, -113.574076),
    'T6H 2J4': (53.489435, -113.494305),
    'T6E 6G4': (53.496501, -113.463115),
    'T5H 2F8': (15.1617382, 105.2236161),
    'T5L 3C5': (53.582267, -113.572812),
    'T6C 4A5': (53.509855, -113.438178),
    'T9E 0R8': (53.291928, -113.530521),
    'T5S 0A6': (53.558529, -113.638664),
    'T6W 1A1': (53.420286, -113.494915),
    'T5L 2G7': (53.571387, -113.566433),
    'T5G 0N9': (53.570162, -113.489783),
    'T5S 1B4': (53.540342, -113.629705),
    'T5S 0A1': (53.559087, -113.624689),
    'T5S 1A7': (53.542411, -113.638397)
    # (all 185 postal codes and coordinates)
}

# 3. Convert dealer_postal_code to string in df
df['dealer_postal_code'] = df['dealer_postal_code'].astype(str)

# 4. Update df with coordinates using the dictionary
df['latitude'] = df['dealer_postal_code'].map(lambda x: postal_code_dict.get(x, (None, None))[0])
df['longitude'] = df['dealer_postal_code'].map(lambda x: postal_code_dict.get(x, (None, None))[1])


# Filter for sold and new vehicles
sold_new_vehicles = df[(df['listing_type'] == 'Sold') & (df['stock_type'] == 'New')]

# Group by postal code and count sold vehicles
postal_code_sales = sold_new_vehicles.groupby('dealer_postal_code')['listing_type'].count().reset_index()
postal_code_sales = postal_code_sales.rename(columns={'listing_type': 'total_vehicles_sold'})



# Filter for sold vehicles
sold_vehicles = df[df['listing_type'] == 'Sold'].copy() # Creating a copy of the slice

# --- Calculate total_vehicles_sold and most_sold_brand ---
dealership_sales = sold_vehicles.groupby('dealer_name')['listing_type'].count().reset_index()
dealership_sales = dealership_sales.rename(columns={'listing_type': 'total_vehicles_sold'})

# Group by brand and dealership, and count sold vehicles
brand_dealership_sales = sold_vehicles.groupby(['make', 'dealer_name'])['listing_type'].count().reset_index()
brand_dealership_sales = brand_dealership_sales.rename(columns={'listing_type': 'brand_vehicles_sold'})

# Find the most sold brand for each dealership
most_sold_brands_by_dealership = brand_dealership_sales.loc[brand_dealership_sales.groupby('dealer_name')['brand_vehicles_sold'].idxmax()]

# Merge with total_vehicles_sold DataFrame
combined_sales = pd.merge(dealership_sales, most_sold_brands_by_dealership[['dealer_name', 'make', 'brand_vehicles_sold']], on='dealer_name', how='left')

# Rename the 'make' column to 'most_sold_brand' for clarity
combined_sales = combined_sales.rename(columns={'make': 'most_sold_brand'})

# --- Merge total_vehicles_sold and most_sold_brand into df ---
# Drop any existing columns to avoid duplicates
df = df.drop(columns=['total_vehicles_sold', 'total_vehicles_sold_x', 'total_vehicles_sold_y', 'most_sold_brand'], errors='ignore')

# Merge the combined_sales data into df
df = pd.merge(df, combined_sales[['dealer_name', 'total_vehicles_sold', 'most_sold_brand']], on='dealer_name', how='left')

# --- Calculate and merge total_revenue ---
sold_vehicles.loc[:, 'revenue'] = sold_vehicles['price']  # Using .loc for assignment
dealership_revenue = sold_vehicles.groupby('dealer_name')['revenue'].sum().reset_index()
dealership_revenue = dealership_revenue.rename(columns={'revenue': 'total_revenue'})

# Drop any existing total_revenue column
df = df.drop(columns=['total_revenue'], errors='ignore')
df = pd.merge(df, dealership_revenue, on='dealer_name', how='left')

# --- Calculate and merge avg_days_on_market ---
average_days_on_market = df.groupby('dealer_name')['days_on_market'].mean().reset_index()
average_days_on_market = average_days_on_market.rename(columns={'days_on_market': 'avg_days_on_market'})

# Drop any existing avg_days_on_market columns
df = df.drop(columns=['avg_days_on_market', 'avg_days_on_market_x', 'avg_days_on_market_y'], errors='ignore')
df = pd.merge(df, average_days_on_market, on='dealer_name', how='left')

# Now convert to integer and then to string
df['avg_days_on_market'] = df['avg_days_on_market'].astype(int).astype(str)

df['total_vehicles_sold'] = df['total_vehicles_sold'].fillna(0)

#Create a MAP to display Region by Postal code
# Assuming 'dealer_city' or another column represents the region, replace 'Region' with that column name
fig = px.scatter_mapbox(
    df,
    lat="latitude",
    lon="longitude",
    color="total_vehicles_sold",
    size="total_vehicles_sold",
    hover_name="dealer_postal_code",
    hover_data=["dealer_city", "total_vehicles_sold"],  # Replace 'Region' with 'dealer_city' or the relevant column
    color_continuous_scale=px.colors.sequential.Viridis,
    size_max=20,  # Adjust marker size as needed
    zoom=9,
    mapbox_style="carto-positron",
    center={"lat": 53.5461, "lon": -113.4938},  # Center on your area
)
fig.update_layout(
    margin={"r": 0, "t": 0, "l": 0, "b": 0},
    mapbox=dict(
        zoom=10,
        style="carto-positron",
        center=dict(lat=53.5461, lon=-113.4938)
    ),

)
fig.show(config={'scrollZoom': True} )


app = dash.Dash(__name__)
app.layout = html.Div([
    html.Div([
        dcc.Input(id="search-input", type="text", placeholder="Search for a dealership..."),
        html.Button("Search", id="search-button")
    ], style={'display': 'flex', 'justifyContent': 'center', 'marginBottom': '10px'}),

    dcc.Graph(id='plotly-map', figure={},
              config={'scrollZoom': True})  # Add config for zoom on click
])
@app.callback(
    Output('plotly-map', 'figure'),
    [Input("search-button", "n_clicks")],  # Triggered by the search button
    [State("search-input", "value")]     # Get the search term
)
def update_plotly_map(n_clicks, search_term):
    filtered_df = df  # Start with the full dataset

    if search_term:  # If a search term is provided
        # Filter the DataFrame based on the search term (dealer_name)
        filtered_df = df[df['dealer_name'].str.contains(search_term, case=False)]

    fig = px.scatter_mapbox(
        filtered_df,
        lat="latitude",
        lon="longitude",
        hover_name="dealer_name",
        size_max=15,
        zoom=11,
        mapbox_style="open-street-map",
        color="total_vehicles_sold",
        color_continuous_scale="Viridis",
        range_color=[0, 3000],
        custom_data=["dealer_postal_code", "total_vehicles_sold", "most_sold_brand", "total_revenue", "avg_days_on_market"]
    )

    min_marker_size = 12
    max_marker_size = 30
    scaling_factor = (max_marker_size - min_marker_size) / max(filtered_df['total_vehicles_sold'])

    fig.update_traces(
        marker=dict(
            size=min_marker_size + filtered_df['total_vehicles_sold'] * scaling_factor,
            opacity=0.7
        ),
        hovertemplate="<br>".join([
            "<b>%{hovertext}</b>",
            '\n',
            "Dealer Postal Code: %{customdata[0]}",
            "Total Vehicles Sold: %{customdata[1]}",
            "Most Sold Brand: %{customdata[2]}",
            "Total Revenue: ${%{customdata[3]:,.0f}}",
            "Average Days on Market: %{customdata[4]} days"
        ])
    )

    fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0},
                      showlegend=True,
                      mapbox=dict(
                          center=dict(lat=53.5461, lon=-113.4938),
                          zoom=9,
                          uirevision='foo'
                      )
                      )

    return fig


if __name__ == '__main__':
    app.run_server(debug=True)
